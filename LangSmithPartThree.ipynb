{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2']=\"true\"\n",
    "os.environ['LANGCHAIN_ENDPOINT']=\"https://api.smith.langchain.com\"\n",
    "os.environ['LANGCHAIN_API_KEY']=\"\"\n",
    "os.environ['LANGCHAIN_PROJECT']=\"rag_demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-representation Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "C:\\Users\\Tasnim\\AppData\\Local\\Temp\\ipykernel_9768\\3754416036.py:12: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import uuid\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from tqdm.autonotebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\")\n",
    "docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like you're discussing the limitations and challenges of using Large Language Models (LLMs) in autonomous agents. Here are some key points and potential areas for discussion:\n",
      "\n",
      "1. **Robustness to errors**: You mentioned that LLMs may make formatting errors or exhibit rebellious behavior, making them less robust compared to humans who learn from trial and error.\n",
      "2. **Reliability of natural language interface**: The current agent system relies on natural language as an interface between LLMs and external components. However, the reliability of model outputs is questionable due to potential formatting errors or misinterpretation of instructions.\n",
      "3. **Need for robustness and steerability**: To build reliable autonomous agents, it's essential to develop methods that can handle unexpected errors and provide a high degree of steerability, allowing humans to guide the agent's behavior when necessary.\n",
      "\n",
      "Some potential areas for discussion:\n",
      "\n",
      "* How can we improve the reliability of LLMs in natural language interfaces?\n",
      "* What techniques or approaches can be used to enhance the robustness of autonomous agents?\n",
      "* How can we balance the need for steerability with the desire for autonomy and decision-making capabilities in AI systems?\n",
      "* What are some potential applications or domains where these challenges are particularly relevant, such as safety-critical systems or high-stakes decision-making?\n",
      "\n",
      "Please let me know if you'd like to explore any of these topics further!\n",
      "It seems like you provided a comprehensive article on the topic of high-quality human data, including various methods and references related to data annotation, quality control, and machine learning. Here's a summary of the main points:\n",
      "\n",
      "**Introduction**\n",
      "\n",
      "The article discusses the importance of high-quality human data in machine learning and AI research.\n",
      "\n",
      "**Data Quality Issues**\n",
      "\n",
      "Francis Galton's 1907 work on \"Vox populi\" highlights the potential for biased opinions among humans. Modern studies have shown that crowd-sourced data can be noisy, leading to suboptimal results in machine learning models.\n",
      "\n",
      "**Annotation Methods**\n",
      "\n",
      "The article discusses various methods for annotating human data, including:\n",
      "\n",
      "1. Amazon's Mechanical Turk: A platform for crowd-sourcing tasks, which has been used for data annotation.\n",
      "2. Two contrasting data annotation paradigms: One emphasizes efficiency and another focuses on quality control.\n",
      "3. Truth Is a Lie: Crowd truth and the seven myths of human annotation.\n",
      "\n",
      "**Quality Control Methods**\n",
      "\n",
      "The article mentions several methods for ensuring high-quality human data, including:\n",
      "\n",
      "1. Noisy Cross-Validation (NCV): A method that identifies clean samples based on predicted labels from a model trained on half of the dataset.\n",
      "2. Iterative NCV (INCV): An iterative version of NCV that adds more clean samples to a trusted candidate set and removes noisy samples.\n",
      "3. Fast, Cheap, and Creative: Evaluating translation quality using Amazon's Mechanical Turk.\n",
      "\n",
      "**References**\n",
      "\n",
      "The article provides 18 references related to data annotation, quality control, and machine learning.\n",
      "\n",
      "Overall, the article highlights the importance of high-quality human data in machine learning research and discusses various methods for ensuring such quality.\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(model=\"llama3.1\")\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document in 5-7 sentences. You response MUST contain only the summarized document. Do NOT add any aditional text or comment or introduction before or after the summarized document.\\nDocument: {doc}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "summaries = chain.batch(docs, {\"max_concurrency\": 5})\n",
    "\n",
    "for summary in summaries:\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The vectorstore to use to index the child chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tasnim\\Anaconda3\\envs\\tasnimllm\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\Tasnim\\Anaconda3\\envs\\tasnimllm\\lib\\site-packages\\sentence_transformers\\models\\Dense.py:89: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(input_path, \"pytorch_model.bin\"), map_location=torch.device(\"cpu\"))\n",
      "c:\\Users\\Tasnim\\Anaconda3\\envs\\tasnimllm\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 0.4. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"dunzhang/stella_en_1.5B_v5\")\n",
    "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The storage layer for the parent documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docs linked to summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    }
   ],
   "source": [
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'doc_id': '3fb456bf-1aa7-456b-9950-a7faea0a536c'}, page_content='It seems like you provided a comprehensive article on the topic of high-quality human data, including various methods and references related to data annotation, quality control, and machine learning. Here\\'s a summary of the main points:\\n\\n**Introduction**\\n\\nThe article discusses the importance of high-quality human data in machine learning and AI research.\\n\\n**Data Quality Issues**\\n\\nFrancis Galton\\'s 1907 work on \"Vox populi\" highlights the potential for biased opinions among humans. Modern studies have shown that crowd-sourced data can be noisy, leading to suboptimal results in machine learning models.\\n\\n**Annotation Methods**\\n\\nThe article discusses various methods for annotating human data, including:\\n\\n1. Amazon\\'s Mechanical Turk: A platform for crowd-sourcing tasks, which has been used for data annotation.\\n2. Two contrasting data annotation paradigms: One emphasizes efficiency and another focuses on quality control.\\n3. Truth Is a Lie: Crowd truth and the seven myths of human annotation.\\n\\n**Quality Control Methods**\\n\\nThe article mentions several methods for ensuring high-quality human data, including:\\n\\n1. Noisy Cross-Validation (NCV): A method that identifies clean samples based on predicted labels from a model trained on half of the dataset.\\n2. Iterative NCV (INCV): An iterative version of NCV that adds more clean samples to a trusted candidate set and removes noisy samples.\\n3. Fast, Cheap, and Creative: Evaluating translation quality using Amazon\\'s Mechanical Turk.\\n\\n**References**\\n\\nThe article provides 18 references related to data annotation, quality control, and machine learning.\\n\\nOverall, the article highlights the importance of high-quality human data in machine learning research and discusses various methods for ensuring such quality.')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Memory in agents\"\n",
    "sub_docs = vectorstore.similarity_search(query,k=1)\n",
    "sub_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tasnim\\Anaconda3\\envs\\tasnimllm\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Thinking about High-Quality Human Data | Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Posts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Archive\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tags\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FAQ\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "emojisearch.app\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Thinking about High-Quality Human Data\n",
      "    \n",
      "Date: February 5, 2024  |  Estimated Reading Time: 21 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "\n",
      "\n",
      "Human Raters ↔ Data Quality\n",
      "\n",
      "The Wisdom of the Crowd\n",
      "\n",
      "Rater Agreement\n",
      "\n",
      "Rater Disagreement & Two Paradigms\n",
      "\n",
      "\n",
      "Data Quality ↔ Model Training\n",
      "\n",
      "Inf\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(query,n_results=1)\n",
    "print(retrieved_docs[0].page_content[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAPTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCEL docs\n",
    "url = \"https://python.langchain.com/docs/expression_language/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# LCEL w/ PydanticOutputParser (outside the primary LCEL docs)\n",
    "url = \"https://python.langchain.com/docs/modules/model_io/output_parsers/quick_start\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs_pydantic = loader.load()\n",
    "\n",
    "# LCEL w/ Self Query (outside the primary LCEL docs)\n",
    "url = \"https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs_sq = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.extend([*docs_pydantic, *docs_sq])\n",
    "docs_texts = [d.page_content for d in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the number of tokens for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "counts = [num_tokens_from_string(d, \"cl100k_base\") for d in docs_texts]\n",
    "\n",
    "# Plotting the histogram of token counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(counts, bins=30, color=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
    "plt.title(\"Histogram of Token Counts\")\n",
    "plt.xlabel(\"Token Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis=\"y\", alpha=0.75)\n",
    "\n",
    "# Display the histogram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc Texts Concatanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens in all context: 5060\n"
     ]
    }
   ],
   "source": [
    "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "d_reversed = list(reversed(d_sorted))\n",
    "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "    [doc.page_content for doc in d_reversed]\n",
    ")\n",
    "print(\n",
    "    \"Num tokens in all context: %s\"\n",
    "    % num_tokens_from_string(concatenated_content, \"cl100k_base\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc Texts Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size_tok = 300\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=chunk_size_tok, chunk_overlap=50\n",
    ")\n",
    "texts_split = text_splitter.split_text(concatenated_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tasnim\\Anaconda3\\envs\\tasnimllm\\lib\\site-packages\\sentence_transformers\\models\\Dense.py:89: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(input_path, \"pytorch_model.bin\"), map_location=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(model=\"llama3.1\")\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"dunzhang/stella_en_1.5B_v5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "RANDOM_SEED = 224  # Fixed seed for reproducibility\n",
    "\n",
    "### --- Code from citations referenced above (added comments and docstrings) --- ###\n",
    "\n",
    "\n",
    "def global_cluster_embeddings(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    n_neighbors: Optional[int] = None,\n",
    "    metric: str = \"cosine\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform global dimensionality reduction on the embeddings using UMAP.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for the reduced space.\n",
    "    - n_neighbors: Optional; the number of neighbors to consider for each point.\n",
    "                   If not provided, it defaults to the square root of the number of embeddings.\n",
    "    - metric: The distance metric to use for UMAP.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
    "    \"\"\"\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=n_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def local_cluster_embeddings(\n",
    "    embeddings: np.ndarray, dim: int, num_neighbors: int = 10, metric: str = \"cosine\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform local dimensionality reduction on the embeddings using UMAP, typically after global clustering.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for the reduced space.\n",
    "    - num_neighbors: The number of neighbors to consider for each point.\n",
    "    - metric: The distance metric to use for UMAP.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
    "    \"\"\"\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=num_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def get_optimal_clusters(\n",
    "    embeddings: np.ndarray, max_clusters: int = 50, random_state: int = RANDOM_SEED\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Determine the optimal number of clusters using the Bayesian Information Criterion (BIC) with a Gaussian Mixture Model.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - max_clusters: The maximum number of clusters to consider.\n",
    "    - random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - An integer representing the optimal number of clusters found.\n",
    "    \"\"\"\n",
    "    max_clusters = min(max_clusters, len(embeddings))\n",
    "    n_clusters = np.arange(1, max_clusters)\n",
    "    bics = []\n",
    "    for n in n_clusters:\n",
    "        gm = GaussianMixture(n_components=n, random_state=random_state)\n",
    "        gm.fit(embeddings)\n",
    "        bics.append(gm.bic(embeddings))\n",
    "    return n_clusters[np.argmin(bics)]\n",
    "\n",
    "\n",
    "def GMM_cluster(embeddings: np.ndarray, threshold: float, random_state: int = 0):\n",
    "    \"\"\"\n",
    "    Cluster embeddings using a Gaussian Mixture Model (GMM) based on a probability threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - threshold: The probability threshold for assigning an embedding to a cluster.\n",
    "    - random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing the cluster labels and the number of clusters determined.\n",
    "    \"\"\"\n",
    "    n_clusters = get_optimal_clusters(embeddings)\n",
    "    gm = GaussianMixture(n_components=n_clusters, random_state=random_state)\n",
    "    gm.fit(embeddings)\n",
    "    probs = gm.predict_proba(embeddings)\n",
    "    labels = [np.where(prob > threshold)[0] for prob in probs]\n",
    "    return labels, n_clusters\n",
    "\n",
    "\n",
    "def perform_clustering(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    threshold: float,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Perform clustering on the embeddings by first reducing their dimensionality globally, then clustering\n",
    "    using a Gaussian Mixture Model, and finally performing local clustering within each global cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for UMAP reduction.\n",
    "    - threshold: The probability threshold for assigning an embedding to a cluster in GMM.\n",
    "\n",
    "    Returns:\n",
    "    - A list of numpy arrays, where each array contains the cluster IDs for each embedding.\n",
    "    \"\"\"\n",
    "    if len(embeddings) <= dim + 1:\n",
    "        # Avoid clustering when there's insufficient data\n",
    "        return [np.array([0]) for _ in range(len(embeddings))]\n",
    "\n",
    "    # Global dimensionality reduction\n",
    "    reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)\n",
    "    # Global clustering\n",
    "    global_clusters, n_global_clusters = GMM_cluster(\n",
    "        reduced_embeddings_global, threshold\n",
    "    )\n",
    "\n",
    "    all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n",
    "    total_clusters = 0\n",
    "\n",
    "    # Iterate through each global cluster to perform local clustering\n",
    "    for i in range(n_global_clusters):\n",
    "        # Extract embeddings belonging to the current global cluster\n",
    "        global_cluster_embeddings_ = embeddings[\n",
    "            np.array([i in gc for gc in global_clusters])\n",
    "        ]\n",
    "\n",
    "        if len(global_cluster_embeddings_) == 0:\n",
    "            continue\n",
    "        if len(global_cluster_embeddings_) <= dim + 1:\n",
    "            # Handle small clusters with direct assignment\n",
    "            local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n",
    "            n_local_clusters = 1\n",
    "        else:\n",
    "            # Local dimensionality reduction and clustering\n",
    "            reduced_embeddings_local = local_cluster_embeddings(\n",
    "                global_cluster_embeddings_, dim\n",
    "            )\n",
    "            local_clusters, n_local_clusters = GMM_cluster(\n",
    "                reduced_embeddings_local, threshold\n",
    "            )\n",
    "\n",
    "        # Assign local cluster IDs, adjusting for total clusters already processed\n",
    "        for j in range(n_local_clusters):\n",
    "            local_cluster_embeddings_ = global_cluster_embeddings_[\n",
    "                np.array([j in lc for lc in local_clusters])\n",
    "            ]\n",
    "            indices = np.where(\n",
    "                (embeddings == local_cluster_embeddings_[:, None]).all(-1)\n",
    "            )[1]\n",
    "            for idx in indices:\n",
    "                all_local_clusters[idx] = np.append(\n",
    "                    all_local_clusters[idx], j + total_clusters\n",
    "                )\n",
    "\n",
    "        total_clusters += n_local_clusters\n",
    "\n",
    "    return all_local_clusters\n",
    "\n",
    "\n",
    "### --- Our code below --- ###\n",
    "\n",
    "\n",
    "def embed(texts):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of text documents.\n",
    "\n",
    "    This function assumes the existence of an `embd` object with a method `embed_documents`\n",
    "    that takes a list of texts and returns their embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], a list of text documents to be embedded.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: An array of embeddings for the given text documents.\n",
    "    \"\"\"\n",
    "    text_embeddings = embedding_function.embed_documents(texts)\n",
    "    text_embeddings_np = np.array(text_embeddings)\n",
    "    return text_embeddings_np\n",
    "\n",
    "\n",
    "def embed_cluster_texts(texts):\n",
    "    \"\"\"\n",
    "    Embeds a list of texts and clusters them, returning a DataFrame with texts, their embeddings, and cluster labels.\n",
    "\n",
    "    This function combines embedding generation and clustering into a single step. It assumes the existence\n",
    "    of a previously defined `perform_clustering` function that performs clustering on the embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], a list of text documents to be processed.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A DataFrame containing the original texts, their embeddings, and the assigned cluster labels.\n",
    "    \"\"\"\n",
    "    text_embeddings_np = embed(texts)  # Generate embeddings\n",
    "    cluster_labels = perform_clustering(\n",
    "        text_embeddings_np, 10, 0.1\n",
    "    )  # Perform clustering on the embeddings\n",
    "    df = pd.DataFrame()  # Initialize a DataFrame to store the results\n",
    "    df[\"text\"] = texts  # Store original texts\n",
    "    df[\"embd\"] = list(text_embeddings_np)  # Store embeddings as a list in the DataFrame\n",
    "    df[\"cluster\"] = cluster_labels  # Store cluster labels\n",
    "    return df\n",
    "\n",
    "\n",
    "def fmt_txt(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Formats the text documents in a DataFrame into a single string.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the 'text' column with text documents to format.\n",
    "\n",
    "    Returns:\n",
    "    - A single string where all text documents are joined by a specific delimiter.\n",
    "    \"\"\"\n",
    "    unique_txt = df[\"text\"].tolist()\n",
    "    return \"--- --- \\n --- --- \".join(unique_txt)\n",
    "\n",
    "\n",
    "def embed_cluster_summarize_texts(\n",
    "    texts: List[str], level: int\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Embeds, clusters, and summarizes a list of texts. This function first generates embeddings for the texts,\n",
    "    clusters them based on similarity, expands the cluster assignments for easier processing, and then summarizes\n",
    "    the content within each cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: A list of text documents to be processed.\n",
    "    - level: An integer parameter that could define the depth or detail of processing.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple containing two DataFrames:\n",
    "      1. The first DataFrame (`df_clusters`) includes the original texts, their embeddings, and cluster assignments.\n",
    "      2. The second DataFrame (`df_summary`) contains summaries for each cluster, the specified level of detail,\n",
    "         and the cluster identifiers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed and cluster the texts, resulting in a DataFrame with 'text', 'embd', and 'cluster' columns\n",
    "    df_clusters = embed_cluster_texts(texts)\n",
    "\n",
    "    # Prepare to expand the DataFrame for easier manipulation of clusters\n",
    "    expanded_list = []\n",
    "\n",
    "    # Expand DataFrame entries to document-cluster pairings for straightforward processing\n",
    "    for index, row in df_clusters.iterrows():\n",
    "        for cluster in row[\"cluster\"]:\n",
    "            expanded_list.append(\n",
    "                {\"text\": row[\"text\"], \"embd\": row[\"embd\"], \"cluster\": cluster}\n",
    "            )\n",
    "\n",
    "    # Create a new DataFrame from the expanded list\n",
    "    expanded_df = pd.DataFrame(expanded_list)\n",
    "\n",
    "    # Retrieve unique cluster identifiers for processing\n",
    "    all_clusters = expanded_df[\"cluster\"].unique()\n",
    "\n",
    "    print(f\"--Generated {len(all_clusters)} clusters--\")\n",
    "\n",
    "    # Summarization\n",
    "    template = \"\"\"Here is a sub-set of LangChain Expression Language doc. \n",
    "    \n",
    "    LangChain Expression Language provides a way to compose chain in LangChain.\n",
    "    \n",
    "    Give a detailed summary of the documentation provided.\n",
    "    \n",
    "    Documentation:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Format text within each cluster for summarization\n",
    "    summaries = []\n",
    "    for i in all_clusters:\n",
    "        df_cluster = expanded_df[expanded_df[\"cluster\"] == i]\n",
    "        formatted_txt = fmt_txt(df_cluster)\n",
    "        summaries.append(chain.invoke({\"context\": formatted_txt}))\n",
    "\n",
    "    # Create a DataFrame to store summaries with their corresponding cluster and level\n",
    "    df_summary = pd.DataFrame(\n",
    "        {\n",
    "            \"summaries\": summaries,\n",
    "            \"level\": [level] * len(summaries),\n",
    "            \"cluster\": list(all_clusters),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df_clusters, df_summary\n",
    "\n",
    "\n",
    "def recursive_embed_cluster_summarize(\n",
    "    texts: List[str], level: int = 1, n_levels: int = 3\n",
    ") -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Recursively embeds, clusters, and summarizes texts up to a specified level or until\n",
    "    the number of unique clusters becomes 1, storing the results at each level.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], texts to be processed.\n",
    "    - level: int, current recursion level (starts at 1).\n",
    "    - n_levels: int, maximum depth of recursion.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[int, Tuple[pd.DataFrame, pd.DataFrame]], a dictionary where keys are the recursion\n",
    "      levels and values are tuples containing the clusters DataFrame and summaries DataFrame at that level.\n",
    "    \"\"\"\n",
    "    results = {}  # Dictionary to store results at each level\n",
    "\n",
    "    # Perform embedding, clustering, and summarization for the current level\n",
    "    df_clusters, df_summary = embed_cluster_summarize_texts(texts, level)\n",
    "\n",
    "    # Store the results of the current level\n",
    "    results[level] = (df_clusters, df_summary)\n",
    "\n",
    "    # Determine if further recursion is possible and meaningful\n",
    "    unique_clusters = df_summary[\"cluster\"].nunique()\n",
    "    if level < n_levels and unique_clusters > 1:\n",
    "        # Use summaries as the input texts for the next level of recursion\n",
    "        new_texts = df_summary[\"summaries\"].tolist()\n",
    "        next_level_results = recursive_embed_cluster_summarize(\n",
    "            new_texts, level + 1, n_levels\n",
    "        )\n",
    "\n",
    "        # Merge the results from the next level into the current results dictionary\n",
    "        results.update(next_level_results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Generated 1 clusters--\n"
     ]
    }
   ],
   "source": [
    "leaf_texts = docs_texts\n",
    "results = recursive_embed_cluster_summarize(leaf_texts, level=1, n_levels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = leaf_texts.copy()\n",
    "\n",
    "# Iterate through the results to extract summaries from each level and add them to all_texts\n",
    "for level in sorted(results.keys()):\n",
    "    # Extract summaries from the current level's DataFrame\n",
    "    summaries = results[level][1][\"summaries\"].tolist()\n",
    "    # Extend all_texts with the summaries from the current level\n",
    "    all_texts.extend(summaries)\n",
    "\n",
    "# Now, use all_texts to build the vectorstore with Chroma\n",
    "vectorstore = Chroma.from_texts(texts=all_texts, embedding=embedding_function)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt & Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided text, it seems that you are asking about the features and benefits of using the LangChain Expression Language (LCEL). However, there is no specific question to answer.\n",
      "\n",
      "If I had to provide an answer based on the context, I would say:\n",
      "\n",
      "The LangChain Expression Language (LCEL) offers a declarative way to compose chains together, making it easy to put prototypes in production with no code changes. It provides features such as first-class streaming support, async support, optimized parallel execution, retries and fallbacks, access to intermediate results, input and output schemas, seamless LangSmith tracing, and seamless LangServe deployment. These features enable building complex chains that can be executed with great performance, handle many concurrent requests, and provide maximum observability and debuggability.\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain.invoke(\"How to define a RAG chain? Give me a specific code example.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tasnimllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
