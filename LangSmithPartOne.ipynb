{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b065647",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0115b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce58ac4",
   "metadata": {},
   "source": [
    "## Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74a019ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2']=\"true\"\n",
    "os.environ['LANGCHAIN_ENDPOINT']=\"https://api.smith.langchain.com\"\n",
    "os.environ['LANGCHAIN_API_KEY']=\"\"\n",
    "os.environ['LANGCHAIN_PROJECT']=\"rag_demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b79876",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db58960c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Neil Armstrong!\n",
      "\n",
      "On July 20, 1969, Neil Armstrong became the first person to set foot on the Moon as part of the Apollo 11 mission. As he stepped off the lunar module Eagle and onto the Moon's surface, he famously declared: \"That's one small step for man, one giant leap for mankind.\"\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(model=\"llama3.1\")\n",
    "print(llm.invoke(\"The first man on the moon was ...\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f4a060",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4a842d",
   "metadata": {},
   "source": [
    "### Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "740cc121",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6a442b",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49b4d4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b61c75a",
   "metadata": {},
   "source": [
    "### Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dae742e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tasnim\\Anaconda3\\envs\\tasnimllm\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\Tasnim\\Anaconda3\\envs\\tasnimllm\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\Tasnim\\Anaconda3\\envs\\tasnimllm\\lib\\site-packages\\sentence_transformers\\models\\Dense.py:89: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(input_path, \"pytorch_model.bin\"), map_location=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    }
   ],
   "source": [
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"dunzhang/stella_en_1.5B_v5\") # OllamaEmbeddings(model=\"llama3\") # dunzhang/stella_en_400M_v5 # dunzhang/stella_en_1.5B_v5\n",
    "print(\"Download Complete.\")\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_function)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404ca334",
   "metadata": {},
   "source": [
    "## Retrieval and Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53df2dd5",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ef3fb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"You are a helpful assistant that generates multiple search queries based on a single input query.\"),\n",
    "#     (\"user\", \"Generate multiple search queries related to: {original_query}\"),\n",
    "#     (\"user\", \"OUTPUT (4 queries):\")\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9945158c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))]\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3137885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(question, context):\n",
    "    prompt_template=f'''You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:'''\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2afdbc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: a\n",
      "Context: b\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(generate_prompt(\"a\", \"b\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fef814",
   "metadata": {},
   "source": [
    "### Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15d9b5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8084123c",
   "metadata": {},
   "source": [
    "### Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2a81b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb41869",
   "metadata": {},
   "source": [
    "### Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f737af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know. \\n\\n(Note: The provided context doesn't mention Task Decomposition, it seems to be about Maximum Inner Product Search and ANN algorithms.)\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad87fdb",
   "metadata": {},
   "source": [
    "## Similarity and Retrieved Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a87d6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04df4b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents\n",
    "question = \"What kinds of pets do I like?\"\n",
    "document = \"My favorite pet is a cat.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6286fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(question, \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab6fb18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "query_result = embedding_function.embed_query(question)\n",
    "document_result = embedding_function.embed_query(document)\n",
    "\n",
    "print(len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41dc332f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.6351015463038355\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "similarity = cosine_similarity(query_result, document_result)\n",
    "print(\"Cosine Similarity:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28f56b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tasnim\\Anaconda3\\envs\\tasnimllm\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
    "# docs = retriever.get_relevant_documents(\"What is Task Decomposition?\")\n",
    "docs = retriever.get_relevant_documents(\"What is Task Decomposition?\")\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5af4dcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Maximum Inner Product Search (MIPS)#'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='A couple common choices of ANN algorithms for fast MIPS:'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Environment information is present in a tree structure.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Conversatin samples:\\n[\\n  {\\n    \"role\": \"system\",')]\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9864ed56",
   "metadata": {},
   "source": [
    "# Query Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760b7bab",
   "metadata": {},
   "source": [
    "## Multi-Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf0ec6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.load import dumps, loads\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed36633",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f9b9476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='You are an AI language model assistant. Your task is to generate five \\ndifferent versions of the given user question to retrieve relevant documents from a vector \\ndatabase. By generating multiple perspectives on the user question, your goal is to help\\nthe user overcome some of the limitations of the distance-based similarity search. \\nProvide these alternative questions separated by newlines. Original question: {question}'))])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "prompt_perspectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb6f9278",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | llm \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28ebea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1addb1",
   "metadata": {},
   "source": [
    "### Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6885d4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tasnim\\Anaconda3\\envs\\tasnimllm\\lib\\site-packages\\langchain_core\\_api\\beta_decorator.py:87: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e60f87",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17984a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Answer the following question based on this context:\\n\\n{context}\\n\\nQuestion: {question}\\n'))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4bc996e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition refers to the process of breaking down large tasks into smaller, manageable subgoals for efficient handling by LLM (Large Language Model) agents. This allows the agent to effectively handle complex tasks and enables efficient planning and execution of tasks.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_query_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "multi_query_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df0b1c6",
   "metadata": {},
   "source": [
    "## RAG Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36efc181",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b28bd31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='You are a helpful assistant that generates multiple search queries based on a single input query. \\n\\nGenerate multiple search queries related to: {question} \\n\\nOutput (4 queries):'))])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_fusion_template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(rag_fusion_template)\n",
    "prompt_rag_fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f370bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e83245b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06c80702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain_rag_fusion = fusion_generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e946fde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition refers to the process of breaking down large tasks into smaller, manageable subgoals that can be efficiently handled by a Large Language Model (LLM) agent. This allows the agent to tackle complex tasks in a more organized and structured manner, leveraging its capabilities to parse and plan tasks effectively.\n"
     ]
    }
   ],
   "source": [
    "fusion_final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(fusion_final_rag_chain.invoke({\"question\":question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a1bd84",
   "metadata": {},
   "source": [
    "## Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2af12874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\\nThe goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. Your question should only contain the generated queries. Do not provide any additional text or explanation before or after the generated queries. Your response MUST contain only the queries, numbered from 1 to 3. \\n\\nGenerate multiple search queries related to: {question} \\n\\nOutput (3 queries):'))])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decomposition_template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. Your question should only contain the generated queries. Do not provide any additional text or explanation before or after the generated queries. Your response MUST contain only the queries, numbered from 1 to 3. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(decomposition_template)\n",
    "prompt_decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1470586",
   "metadata": {},
   "source": [
    "### Generate Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "035a870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "00f9a3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is the role of a knowledge graph in an LLM-powered autonomous agent system?\n",
      "2. How does a natural language understanding component contribute to the functionality of an LLM-powered autonomous agent system?\n",
      "3. What are the key characteristics of a decision-making module in an LLM-powered autonomous agent system?\n"
     ]
    }
   ],
   "source": [
    "for ques in questions:\n",
    "    print(ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17d31faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'q_a_pairs', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'q_a_pairs', 'question'], template='Here is the question you need to answer:\\n\\n\\n --- \\n {question} \\n --- \\n\\n\\nHere is any available background question + answer pairs:\\n\\n\\n --- \\n {q_a_pairs} \\n --- \\n\\n\\nHere is additional context relevant to the question: \\n\\n\\n --- \\n {context} \\n --- \\n\\n\\nUse the above context and any background question + answer pairs to answer the question: \\n {question}\\n'))])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decomposition_qa_pair_template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt_qa_pair = ChatPromptTemplate.from_template(decomposition_qa_pair_template)\n",
    "\n",
    "decomposition_prompt_qa_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1127da65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "405b5663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context and the relevant background information, I will attempt to answer the question.\n",
      "\n",
      "The key characteristics of a decision-making module in an LLM-powered autonomous agent system can be inferred by looking at the role of tool use, natural language understanding (NLU), and knowledge graph integration in such systems.\n",
      "\n",
      "From the context provided, it appears that the decision-making module plays a crucial role in enabling the LLM-powered autonomous agent system to make informed decisions based on user input, external information gathered through tool calls, and the system's internal model weights.\n",
      "\n",
      "The mention of Boiko et al. (2023) suggests that the agent uses tools \"to browse the Internet, read documentation, execute code, call robotics experimentation APIs\" and more. This implies that the decision-making module is responsible for deciding which external tools or APIs to utilize based on user input and the system's internal understanding.\n",
      "\n",
      "The answer can also be found by looking at the relevant background information provided in question 2, where it was mentioned that the NLU component contributes significantly to the functionality of an LLM-powered autonomous agent system. This implies that the decision-making module must take into account the output of the NLU component when making decisions.\n",
      "\n",
      "Given this context, the key characteristics of a decision-making module in an LLM-powered autonomous agent system are:\n",
      "\n",
      "1. **Informed decision-making**: The decision-making module makes informed decisions based on user input, external information gathered through tool calls, and the system's internal model weights.\n",
      "2. **Contextual understanding**: The module takes into account the output of the NLU component to understand the context of the user input and make appropriate decisions.\n",
      "3. **External tool utilization**: The decision-making module decides which external tools or APIs to utilize based on user input and the system's internal understanding.\n",
      "4. **Efficient task completion**: The module aims to complete tasks in the least number of steps, as mentioned in the provided context.\n",
      "\n",
      "In summary, the key characteristics of a decision-making module in an LLM-powered autonomous agent system are informed decision-making, contextual understanding, external tool utilization, and efficient task completion.\n"
     ]
    }
   ],
   "source": [
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    \n",
    "    rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \n",
    "     \"question\": itemgetter(\"question\"),\n",
    "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")} \n",
    "    | decomposition_prompt_qa_pair\n",
    "    | llm\n",
    "    | StrOutputParser())\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs}) \n",
    "    q_a_pair = format_qa_pair(q,answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45ae31b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_rag(question,prompt_rag,sub_question_generator_chain):\n",
    "    \"\"\"RAG on each sub-question\"\"\"\n",
    "    \n",
    "    # Use our decomposition / \n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n",
    "    \n",
    "    # Initialize a list to hold RAG chain results\n",
    "    rag_results = []\n",
    "    \n",
    "    for sub_question in sub_questions:\n",
    "        \n",
    "        # Retrieve documents for each sub-question\n",
    "        retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
    "        \n",
    "        # Use retrieved documents and sub-question in RAG chain\n",
    "        answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs, \n",
    "                                                                \"question\": sub_question})\n",
    "        rag_results.append(answer)\n",
    "    \n",
    "    return rag_results,sub_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "154eec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "answers, questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01738949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, I would say that the main components of an LLM-powered autonomous agent system include:\n",
      "\n",
      "1. **Large Language Model (LLM)**: The core component that provides the ability to understand and generate human-like text.\n",
      "2. **External API access**: Allows the agent to gather extra information from external sources.\n",
      "3. **Current data access**: Enables the agent to leverage current data for informed decision-making.\n",
      "4. **Code execution capabilities**: Empowers the agent to execute code and perform tasks that may not be possible with just the LLM's abilities.\n",
      "5. **Proprietary source utilization**: Allows the agent to tap into proprietary sources of information, if available.\n",
      "\n",
      "These components work together to enable autonomous agents to make informed decisions and take actions accordingly.\n"
     ]
    }
   ],
   "source": [
    "def format_qa_pairs_individual(questions, answers):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs_individual(questions, answers)\n",
    "\n",
    "# Prompt\n",
    "template_individual = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "individual_final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(individual_final_rag_chain.invoke({\"context\":context,\"question\":question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cdf9e4",
   "metadata": {},
   "source": [
    "## Step Back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bde0c4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4eb8192d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question'] messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:')), FewShotChatMessagePromptTemplate(examples=[{'input': 'Could the members of The Police perform lawful arrests?', 'output': 'what can the members of The Police do?'}, {'input': 'Jan Sindel’s was born in what country?', 'output': 'what is Jan Sindel’s personal history?'}], example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], template='{output}'))])), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='Generate Step-back question for this Question: {question}. You MUST provide only the step-back question. Do not provide any additional text before or after the question. Be concise.'))]\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"input\": \"A hotel employs cleaners and receptionists. Cleaners earn $500 per week and receptionists earn $350 per week. The hotel requires a minimum of 100 workers of whom at least 20 must be receptionists. To keep the hotel clean and running smoothly, the number of receptionists should be at least a third of the number of cleaners. The hotel wants to keep the weekly wage bill below $30000. Formulate a LP to minimize the wage bill.\",\n",
    "        \"output\": \"How can linear programming be applied to determine the optimal number of employees in different roles to minimize costs under given constraints?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"You are playing a game where a short shot is worth 2 points and a long shot is worth 5 points. In total, you can take at most 14 shots. You must take at least 5 short shots and 2 long shots, but time restricts taking more than 8 of either type. How many of each shot must you take, assuming all your shots get points, to maximize your score? What is your maximum score?\",\n",
    "        \"output\": \"How can linear programming be used to maximize the total score in a game with constraints on the number and type of shots taken?\",\n",
    "    },\n",
    "]\n",
    "# We now transform these to example messages\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        # Few shot examples\n",
    "        few_shot_prompt,\n",
    "        # New question\n",
    "        (\"user\", \"Generate Step-back question for this Question: {question}. You MUST provide only the step-back question. Do not provide any additional text before or after the question. Be concise.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3b32d0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the fundamental concepts involved in training large language models?\n"
     ]
    }
   ],
   "source": [
    "generate_queries_step_back = prompt | llm | StrOutputParser()\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "\n",
    "print(generate_queries_step_back.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ad88cc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition refers to the process of breaking down complex tasks into smaller, more manageable sub-tasks that can be executed by Large Language Model (LLM) agents. This approach involves identifying the key components or steps required to complete a specific task and then creating separate prompts or inputs for each sub-task.\n",
      "\n",
      "By decomposing tasks in this way, LLM agents can focus on one aspect of the task at a time, which can improve their performance, efficiency, and overall quality of output. Task decomposition is particularly useful when working with complex or open-ended tasks that require multiple steps or interactions to complete.\n",
      "\n",
      "In the context of LLMs, task decomposition can involve several key components:\n",
      "\n",
      "1. **Sub-task identification**: Identifying the individual sub-tasks required to complete a larger task.\n",
      "2. **Prompt engineering**: Crafting specific prompts or inputs for each sub-task that can be executed by the LLM agent.\n",
      "3. **Model selection**: Choosing the most suitable LLM model or configuration for each sub-task based on its complexity and requirements.\n",
      "4. **Output integration**: Combining the outputs from each sub-task to generate a complete response or solution.\n",
      "\n",
      "Task decomposition can be applied in various domains, including but not limited to:\n",
      "\n",
      "* Conversational AI: Decomposing complex conversations into smaller sub-conversations or tasks that can be handled by LLMs.\n",
      "* Information retrieval: Breaking down information-seeking tasks into smaller sub-tasks, such as searching for specific data or answering questions.\n",
      "* Content generation: Decomposing content creation tasks into smaller sub-tasks, such as generating text, images, or videos.\n",
      "\n",
      "By applying task decomposition principles, developers and researchers can design more effective LLM-based systems that are capable of handling complex tasks with greater accuracy and efficiency.\n"
     ]
    }
   ],
   "source": [
    "# Response prompt \n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "step_back_chain = (\n",
    "    {\n",
    "        # Retrieve context using the normal question\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        # Retrieve context using the step-back question\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        # Pass on the question\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(step_back_chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d094ece0",
   "metadata": {},
   "source": [
    "## HyDE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f192b47f",
   "metadata": {},
   "source": [
    "### HyDE document generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7d29ad8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='Please write a scientific paper passage to answer the question. You output should contain only the scientific paper passage. DO NOT provide any additional text before or after it.\\nQuestion: {question}\\nPassage:'))]\n"
     ]
    }
   ],
   "source": [
    "hyde_template = \"\"\"Please write a scientific paper passage to answer the question. You output should contain only the scientific paper passage. DO NOT provide any additional text before or after it.\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "prompt_hyde = ChatPromptTemplate.from_template(hyde_template)\n",
    "\n",
    "print(prompt_hyde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "513854a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is a crucial concept in the development and evaluation of Large Language Model (LLM) agents, particularly those designed for complex tasks such as conversational dialogue systems, question-answering, and text summarization. In essence, task decomposition refers to the process of breaking down a given task into smaller, manageable sub-tasks that can be individually addressed by the LLM agent.\n",
      "\n",
      "The primary motivation behind task decomposition in LLM agents is to facilitate more efficient learning and processing of complex tasks. By decomposing a task into smaller components, developers can design more targeted training data and evaluation protocols, which can lead to improved model performance and reduced computational requirements. Moreover, task decomposition enables researchers to identify and address specific knowledge gaps or biases within the model, thereby enhancing its overall reliability and generalizability.\n",
      "\n",
      "Task decomposition in LLM agents typically involves three key stages: task definition, sub-task identification, and task integration. The first stage involves clearly defining the original task and identifying its key components. The second stage requires analyzing these components to determine the most effective way to break them down into smaller, more manageable sub-tasks. Finally, the third stage involves integrating the results of each sub-task to produce a comprehensive solution to the original problem.\n",
      "\n",
      "Effective task decomposition is critical for the development and evaluation of LLM agents, as it enables researchers to create models that are not only more efficient but also more accurate and reliable in their performance. By understanding how different components of complex tasks interact and contribute to overall model behavior, developers can design more effective training protocols and evaluation metrics, which can ultimately lead to significant improvements in the quality and reliability of LLM agents.\n"
     ]
    }
   ],
   "source": [
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde | llm | StrOutputParser() \n",
    ")\n",
    "\n",
    "# Run\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "print(generate_docs_for_retrieval.invoke({\"question\":question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf1ad5d",
   "metadata": {},
   "source": [
    "### Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3853a4ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='The system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve\n",
    "retrieval_chain = generate_docs_for_retrieval | retriever \n",
    "retireved_docs = retrieval_chain.invoke({\"question\":question})\n",
    "retireved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e0b52180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the provided context, task decomposition refers to the process of breaking down large tasks into smaller, manageable subgoals by an LLM (Large Language Model) agent. This allows for efficient handling of complex tasks.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "hyde_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "hyde_rag_chain.invoke({\"context\":retireved_docs,\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b692b0",
   "metadata": {},
   "source": [
    "# Routing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54e47e4",
   "metadata": {},
   "source": [
    "## Logical Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8a662108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import Literal\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41003c59",
   "metadata": {},
   "source": [
    "### Data Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "51f6b4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "datasource: Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(\n",
    "    ...,\n",
    "    description=\"Given a user question choose which datasource would be most relevant for answering their question\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8138eb3f",
   "metadata": {},
   "source": [
    "### LLM for Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "630977e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
    "\n",
    "Based on the programming language the question is referring to, route it to the relevant data source.\n",
    "DataSources are: [python_docs, js_docs, golang_docs]\n",
    "Your response should contain only the data source name. DO NOT provide any additional text before or after the data source name.\"\"\"\n",
    "\n",
    "structured_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define router \n",
    "router = structured_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ec529047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python_docs\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Why doesn't the following code work:\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
    "prompt.invoke(\"french\")\n",
    "\"\"\"\n",
    "\n",
    "result = router.invoke({\"question\": question})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9d1baf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_route(result):\n",
    "    if \"python_docs\" in result.lower():\n",
    "        ### Logic here \n",
    "        return \"chain for python_docs\"\n",
    "    elif \"js_docs\" in result.lower():\n",
    "        ### Logic here \n",
    "        return \"chain for js_docs\"\n",
    "    else:\n",
    "        ### Logic here \n",
    "        return \"golang_docs\"\n",
    "\n",
    "router_chain = router | RunnableLambda(choose_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ebf6de29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chain for python_docs\n"
     ]
    }
   ],
   "source": [
    "print(router_chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fde548f",
   "metadata": {},
   "source": [
    "## Semantic Routing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3352a190",
   "metadata": {},
   "source": [
    "### Two prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2eb07aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
    "You are so good because you are able to break down hard problems into their component parts, \\\n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fb5d1d",
   "metadata": {},
   "source": [
    "### Embed Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a14d23b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_templates = [physics_template, math_template]\n",
    "prompt_embeddings = embedding_function.embed_documents(prompt_templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a720f6",
   "metadata": {},
   "source": [
    "### Route Question to Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1df72a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_router(input):\n",
    "    # Embed question\n",
    "    query_embedding = embedding_function.embed_query(input[\"query\"])\n",
    "    # Compute similarity\n",
    "    s_1 = cosine_similarity(query_embedding, embedding_function.embed_query(math_template))\n",
    "    s_2 = cosine_similarity(query_embedding, embedding_function.embed_query(physics_template))\n",
    "    if(s_1>=s_2):\n",
    "        most_similar=math_template\n",
    "    else:\n",
    "        most_similar=physics_template\n",
    "    print(most_similar)\n",
    "    return PromptTemplate.from_template(most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "17457288",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_chain = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_router)\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8a9921a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a very good mathematician. You are great at answering math questions. You are so good because you are able to break down hard problems into their component parts, answer the component parts, and then put them together to answer the broader question.\n",
      "\n",
      "Here is a question:\n",
      "{query}\n",
      "To find the equation of Root Mean Squared Error (RMSE), we can start by breaking it down into its component parts:\n",
      "\n",
      "1. **Mean**: The mean is the average value of a set of numbers. It's calculated by summing up all the values and dividing by the total count.\n",
      "\n",
      "Mathematically, if we have a set of N values, {x1, x2, ..., xN}, then the mean (μ) is:\n",
      "\n",
      "μ = (x1 + x2 + ... + xN) / N\n",
      "\n",
      "2. **Variance**: The variance measures how spread out the numbers are from their mean value.\n",
      "\n",
      "For our set of values {x1, x2, ..., xN}, we calculate the variance (σ²) as:\n",
      "\n",
      "σ² = [(x1 - μ)² + (x2 - μ)² + ... + (xN - μ)²] / N\n",
      "\n",
      "3. **Root**: Taking the square root of a value is denoted by √.\n",
      "\n",
      "4. **Mean Squared Error** (MSE): The Mean Squared Error is the average of the squared differences between predicted and actual values.\n",
      "\n",
      "If we have a set of predicted values, {y1, y2, ..., yN}, and a corresponding set of actual values, {x1, x2, ..., xN}, then the MSE is:\n",
      "\n",
      "MSE = [(y1 - x1)² + (y2 - x2)² + ... + (yN - xN)²] / N\n",
      "\n",
      "5. **Root Mean Squared Error** (RMSE): Finally, we find RMSE by taking the square root of MSE.\n",
      "\n",
      "Mathematically, if {y1, y2, ..., yN} are our predicted values and {x1, x2, ..., xN} are the actual values, then:\n",
      "\n",
      "RMSE = √[[(y1 - x1)² + (y2 - x2)² + ... + (yN - xN)²] / N]\n",
      "\n",
      "Putting all these component parts together gives us the equation for RMSE.\n"
     ]
    }
   ],
   "source": [
    "print(semantic_chain.invoke(\"What is the equation of root mean squared error?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf1d450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
